{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from google.oauth2 import service_account\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## CHANGE DIRECTORY TO WHERE THE RAW DATA IS LOCATED\n",
        "os.chdir(\"/anonymized_path/sakshikumar/Documents/UCD/04. SP 2025/464 Practicum/KWSM finale/FINAL FILES\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AHrefs Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ah = pd.read_excel('organickeywords.xlsx')\n",
        "ah.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processing and Augmenting AHrefs Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter out the blog web\n",
        "ah = ah[~ah['Current URL'].str.startswith(\"https://kwsmdigital.com/blog/\")].reset_index(drop = True)\n",
        "\n",
        "# create an intent metrics\n",
        "ah[['Navigational', 'Informational', 'Commercial', 'Transactional']] = ah[\n",
        "    ['Navigational', 'Informational', 'Commercial', 'Transactional']\n",
        "].astype(int)\n",
        "\n",
        "# High-intent\n",
        "ah[\"intent\"] = ((ah[\"Commercial\"] == 1) | (ah[\"Transactional\"] == 1)).astype(int)\n",
        "\n",
        "# Number of words\n",
        "ah['Word_Count'] = ah['Keyword'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# SERP_feature\n",
        "ah['Has_SERP_Feature'] = ah['SERP features'].apply(lambda x: 1 if pd.notna(x) and x.strip() != '' else 0)\n",
        "\n",
        "# Extract all data\n",
        "ah = ah[['Keyword', 'Current URL', 'KD', 'Volume','CPC', 'Organic traffic', 'intent', 'Navigational', 'Informational',\t'Commercial', 'Transactional', 'Word_Count', \t'Has_SERP_Feature']].rename(columns = {'Current URL':'URL', 'Keyword':'keyword'})\n",
        "\n",
        "ah"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the target columns expected in AHREFS_DATA.csv\n",
        "target_columns = [\n",
        "    'keyword', 'URL', 'KD', 'Volume', 'CPC', 'Organic traffic',\n",
        "    'intent', 'Navigational', 'Informational', 'Commercial', 'Transactional',\n",
        "    'Word_Count', 'Has_SERP_Feature', 'Clicks', 'Impressions', 'CTR', 'Position'\n",
        "]\n",
        "\n",
        "# Initialize missing columns with default values\n",
        "# Assuming default for Clicks, Impressions, CTR, and Position is 0 or NaN if appropriate\n",
        "missing_defaults = {\n",
        "    'Clicks': 0,\n",
        "    'Impressions': 0,\n",
        "    'CTR': 0.0,\n",
        "    'Position': 0.0\n",
        "}\n",
        "\n",
        "for col in target_columns:\n",
        "    if col not in ah.columns:\n",
        "        default_value = missing_defaults.get(col, None)\n",
        "        ah[col] = default_value\n",
        "\n",
        "# Reorder the columns to match the expected AHREFS_DATA.csv format\n",
        "ahrefs_processed = ah[target_columns]\n",
        "\n",
        "ahrefs_processed.shape\n",
        "\n",
        "# Save the new dataframe\n",
        "# ahrefs_processed.to_csv('AHREFS_DATA.csv', index=False)\n",
        "\n",
        "# print(\"New dataset saved as 'AHREFS_DATA.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GSC Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fetching data via API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1\ufe0f\u20e3 Authenticate API\n",
        "SERVICE_ACCOUNT_FILE = \"client_secret_gsc.json\"  # Path to your JSON key file\n",
        "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    SERVICE_ACCOUNT_FILE, scopes=SCOPES\n",
        ")\n",
        "service = build(\"searchconsole\", \"v1\", credentials=credentials)\n",
        "\n",
        "# 2\ufe0f\u20e3 Set API request parameters\n",
        "REQUEST_TEMPLATE = {\n",
        "    \"startDate\": \"2024-01-24\",  # Last 12 months\n",
        "    \"endDate\": \"2025-01-24\",\n",
        "    \"dimensions\": [\"query\", \"page\"],  # Get keyword + URL data\n",
        "    \"rowLimit\": 25000  # GSC API limit: max 25,000 rows\n",
        "}\n",
        "\n",
        "# 3\ufe0f\u20e3 Handle API pagination\n",
        "def fetch_gsc_data():\n",
        "    all_data = []\n",
        "    start_row = 0  # Start from row 0\n",
        "\n",
        "    while True:\n",
        "        request = REQUEST_TEMPLATE.copy()\n",
        "        request[\"startRow\"] = start_row  # Pagination parameter\n",
        "\n",
        "        response = service.searchanalytics().query(siteUrl=\"https://kwsmdigital.com/\", body=request).execute()\n",
        "\n",
        "        if \"rows\" in response:\n",
        "            for row in response[\"rows\"]:\n",
        "                query, page = row[\"keys\"]\n",
        "                clicks = row.get(\"clicks\", 0)\n",
        "                impressions = row.get(\"impressions\", 0)\n",
        "                ctr = row.get(\"ctr\", 0)\n",
        "                position = row.get(\"position\", 0)\n",
        "\n",
        "                # \ud83d\udd39 Filter out pages that start with \"https://kwsmdigital.com/blog\"\n",
        "                if not page.startswith(\"https://kwsmdigital.com/blog\"):\n",
        "                    all_data.append([query, page, clicks, impressions, ctr, position])\n",
        "\n",
        "            start_row += len(response[\"rows\"])  # Move to next page\n",
        "            print(f\"\u2705 Retrieved {len(response['rows'])} rows, total {len(all_data)} rows (after filtering blog pages)...\")\n",
        "\n",
        "            if len(response[\"rows\"]) < 25000:\n",
        "                break  # If fewer than 25,000 rows, all data has been fetched\n",
        "        else:\n",
        "            print(\"\u26a0\ufe0f No more data returned.\")\n",
        "            break\n",
        "\n",
        "    return all_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Function to fetch data from GSC\n",
        "# This function retrieves data from Google Search Console, filtering out blog pages.\n",
        "def fetch_gsc_data():\n",
        "    all_data = []\n",
        "    start_row = 0  # Start from row 0\n",
        "\n",
        "    while True:\n",
        "        request = REQUEST_TEMPLATE.copy()\n",
        "        request[\"startRow\"] = start_row  # Pagination parameter\n",
        "\n",
        "        response = service.searchanalytics().query(siteUrl=\"https://kwsmdigital.com/\", body=request).execute()\n",
        "\n",
        "        if \"rows\" in response:\n",
        "            for row in response[\"rows\"]:\n",
        "                query, page = row[\"keys\"]\n",
        "                clicks = row.get(\"clicks\", 0)\n",
        "                impressions = row.get(\"impressions\", 0)\n",
        "                ctr = row.get(\"ctr\", 0)\n",
        "                position = row.get(\"position\", 0)\n",
        "\n",
        "                # \ud83d\udd39 Filter out pages that start with \"https://kwsmdigital.com/blog\"\n",
        "                if not page.startswith(\"https://kwsmdigital.com/blog\"):\n",
        "                    all_data.append([query, page, clicks, impressions, ctr, position])\n",
        "\n",
        "            start_row += len(response[\"rows\"])  # Move to next page\n",
        "            print(f\"\u2705 Retrieved {len(response['rows'])} rows, total {len(all_data)} rows (after filtering blog pages)...\")\n",
        "\n",
        "            if len(response[\"rows\"]) < 25000:\n",
        "                break  # If fewer than 25,000 rows, all data has been fetched\n",
        "        else:\n",
        "            print(\"\u26a0\ufe0f No more data returned.\")\n",
        "            break\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Run API query\n",
        "data = fetch_gsc_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Convert to DataFrame\n",
        "df_gsc = pd.DataFrame(data, columns=[\"Keyword\", \"URL\", \"Clicks\", \"Impressions\", \"CTR\", \"Position\"])\n",
        "\n",
        "df_gsc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_gsc.to_csv('temp_gsc.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Remove Keyword column\n",
        "df_gsc = df_gsc[[\"URL\", \"Clicks\", \"Impressions\", \"CTR\", \"Position\"]]\n",
        "\n",
        "# 3. Filter for meaningful data (optional but good practice)\n",
        "df_gsc = df_gsc[(df_gsc['Clicks'] > 0) & (df_gsc['Impressions'] > 0)]\n",
        "# Step 4: Sort by Clicks descending\n",
        "gsc_aggregated = df_gsc.sort_values(by='Clicks', ascending=False)\n",
        "\n",
        "# Step 5: Optional - keep top N if needed\n",
        "gsc_aggregated = gsc_aggregated.head(155)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Save to CSV\n",
        "## \t\u2022\tDue to aggregation, the resulting dataset structure may differ slightly from prior manually curated datasets.\n",
        "##\t\u2022\tThis process ensures full reproducibility for future GSC data pulls.\n",
        "gsc_aggregated.to_csv('GSC_DATA_v2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merging All 3 Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load GA4 Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metr_1 = pd.read_csv(\"GA4_DATA.csv\", skiprows=6)\n",
        "metr_1.reset_index(inplace=True)\n",
        "metr_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metr_1.drop(index=0, inplace=True)\n",
        "metr_1 = metr_1.iloc[:,:-1]\n",
        "metr_1.columns = ['Landing page', 'Sessions', 'Active users', 'New users',\n",
        "       'Returning users', 'Total users', 'Event count',\n",
        "       'Average engagement time per session', 'Bounce rate',\n",
        "       'Views per session', 'Engaged sessions per active user']\n",
        "metr_1['Landing page'] = \"https://kwsmdigital.com\" + metr_1['Landing page']\n",
        "metr_1.rename(columns = {\"Landing page\": \"URL\"}, inplace=True)\n",
        "metr_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load GSC Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gsc_page = pd.read_csv(\"GSC_DATA.csv\")\n",
        "gsc_page['URL'] = gsc_page['URL'].str.rstrip('/')\n",
        "gsc_page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gsc_page.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load AHREFS Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ahref = pd.read_csv(\"AHREFS_DATA.csv\")\n",
        "ahref['URL'] = ahref['URL'].str.rstrip('/')\n",
        "ahref.rename(columns={\"keyword\":\"Keyword\"}, inplace=True)\n",
        "ahref = ahref.iloc[:, :-4]\n",
        "ahref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ahref.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Mapping Table to help Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "map_tb = pd.read_csv(\"MAPPING_TABLE.csv\")\n",
        "map_tb['URL'] = map_tb['URL'].str.rstrip('/')\n",
        "map_tb.drop(columns='Source', inplace=True)\n",
        "map_tb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Merge all 3 Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merge1 = pd.merge(metr_1, gsc_page, on=\"URL\", how=\"inner\")\n",
        "merge1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_mappings = pd.merge(\n",
        "    merge1,\n",
        "    map_tb,\n",
        "    on='URL',\n",
        "    how='inner'\n",
        ")\n",
        "valid_mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merge2_exploded = valid_mappings.explode('Keyword')\n",
        "merge2_exploded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_merged = pd.merge(\n",
        "    merge2_exploded,\n",
        "    ahref,\n",
        "    on=['URL', 'Keyword'],\n",
        "    how='inner'\n",
        ")\n",
        "print(final_merged.shape)\n",
        "final_merged.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Display Final Merged Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_merged.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Checks to Ensure No Missing Values and Data Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = final_merged.select_dtypes(include=['float64', 'int64']).columns\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "final_merged[numeric_cols] = imputer.fit_transform(final_merged[numeric_cols])\n",
        "final_merged.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Previewing Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(final_merged.describe().T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Checking Correlation of all Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "correlation = final_merged.select_dtypes(include=['float64', 'int64']).corr()\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_highly_correlated_variables(df, threshold):\n",
        "    corr_matrix = df.corr().abs()\n",
        "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    \n",
        "    highly_correlated = upper_triangle.stack().sort_values(ascending=False)\n",
        "    highly_correlated = highly_correlated[highly_correlated > threshold]\n",
        "    \n",
        "    return highly_correlated\n",
        "\n",
        "highly_correlated_df = get_highly_correlated_variables(correlation, 0.7)\n",
        "print(highly_correlated_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Feature Engineering for Modelling Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_merged['Commercial_Trans_Ratio'] = final_merged['Commercial'] / (final_merged['Transactional'] + 1)  # Add 1 to avoid division by zero\n",
        "final_merged['Intent_Value_Score'] = final_merged['Commercial'] * 0.4 + final_merged['Transactional'] * 0.6  # Weighting based on conversion value\n",
        "final_merged['Intent_KD_Ratio'] = final_merged['intent'] / (final_merged['KD'] + 1)  # Value-to-difficulty ratio\n",
        "final_merged['Volume_Intent_Interaction'] = final_merged['Volume'] * final_merged['intent']\n",
        "final_merged['Position_Inverse'] = 100 / (final_merged['Position'] + 1)  # Transform position to a \"higher is better\" metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Export to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_merged.to_csv(\"dataset.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}